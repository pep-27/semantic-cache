# ğŸ§  SemanticCache: Context-Aware Caching for Multi-Turn LLM Conversations

This project implements a **context-aware semantic caching system** for multi-turn conversations powered by the **Google Gemini API**.
It reuses previous responses that are semantically similar to new queries, reducing API calls, latency, and cost â€” while maintaining contextual accuracy through multi-stage verification.

---

## âš™ï¸ 1. Overview

The system simulates a multi-turn chat session where each user query is:

1. Embedded into a semantic vector using **Gemini Embeddings API**
2. Compared against previous queries in a **vector store**
3. Validated through lexical filters
4. Either reuses a cached response (if similar) or calls Gemini again (if new)

This structure mimics real-world retrieval logic for **LLM-based memory systems**, enabling efficient reuse of semantically close content.

---

## ğŸ§© 2. Architecture (File Structure)

```
semantic_cache/
â”œâ”€â”€ cache/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ embedding_cache.json           # Stores text embeddings locally
â”‚   â”œâ”€â”€ responses_cache.json           # Stores raw LLM responses (dev use only)
â”‚   â”œâ”€â”€ semantic_cache.py              # Core caching logic
â”‚   â””â”€â”€ session_manager.py             # Tracks conversation sessions
â”‚
â”œâ”€â”€ embedding/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ gemini_embedder.py             # Generates embeddings using Gemini API
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ metrics.py                     # Logs performance metrics (hit rate, latency)
â”‚   â””â”€â”€ threshold_sweep.py             # Sweeps similarity thresholds for tuning
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ session_log.json               # Stores full session activity logs
â”‚
â”œâ”€â”€ storage/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ vector_store.py                # Stores and retrieves embedding vectors
â”‚
â”œâ”€â”€ .env                               # Environment variables (Gemini API key)
â”œâ”€â”€ .env.example                       # Example environment configuration
â”œâ”€â”€ .gitignore                         # Git ignore settings
â”œâ”€â”€ main.py                            # Entry point for running the system
â”œâ”€â”€ README.md                          # Project documentation
â””â”€â”€ requirements.txt                   # Python dependencies
```

---

## ğŸ”§ 3. Configuration

| Parameter        | Description                                     | Default |
| ---------------- | ----------------------------------------------- | ------- |
| `threshold`      | Maximum vector distance for semantic similarity | `0.3`   |
| `min_overlap`    | Minimum lexical overlap between queries         | `0.10`  |
| `topk`           | Number of nearest candidates to consider        | `3`     |
| `history_window` | Number of recent turns used for context         | `6`     |

Set your Gemini API key in a `.env` file:

```bash
GEMINI_API_KEY=your_actual_key_here
```

---

## ğŸ” 4. LLM Responses (Real + Locally Cached)

Responses are generated using **`models/gemini-2.5-flash-lite`**.
To minimize development cost, each exact query result is stored locally in:

```
cache/responses_cache.json
```

If the same query appears again, the system retrieves the stored response instead of re-calling Gemini â€” ensuring **zero-cost reuse during debugging**.

This local cache is for **development efficiency only**, not part of the semantic caching mechanism.

---

## ğŸ§  5. Embeddings with Gemini API

Embeddings are generated using **`models/embedding-001`**, via:

```python
genai.embed_content(model="models/embedding-001", content=text)
```

To avoid redundant embedding calls, each textâ€™s embedding is cached locally in:

```
cache/embedding_cache.json
```

âœ… If the same text is embedded again, the system automatically loads its vector from the local cache instead of re-calling Gemini.
When additional conversation history is included, a new contextual embedding is generated to reflect the updated dialogue context.

---

## ğŸš€ 6. SemanticCache Logic

SemanticCache combines **two verification layers** before confirming a cache hit:

1. **Vector Distance Filter** â€” Check if semantic distance < `threshold`
2. **Lexical Overlap Filter** â€” Require minimum token overlap > `min_overlap`

If both pass, a cached answer is reused. Otherwise, a fresh Gemini API call is made.

---

## ğŸ“Š 7. Metrics & Logging

All runs automatically generate:

```
logs/session_log.json
```

Each record includes:

```json
{
  "query": "Effect of global warming on corn productivity",
  "status": "HIT",
  "matched_query": "What is the impact of climate change on corn yields?",
  "distance": 0.14,
  "overlap": 0.42,
  "latency_sec": 0.62
}
```

At the end of each session:

```
ğŸ“Š System Results:
{'total_calls': 8, 'total_hits': 5, 'hit_rate': 0.625, 'avg_latency': 0.48}
ğŸ—‚ï¸ Logs saved to: logs/session_log.json
```

---

## ğŸ§ª 8. Example Run

```
ğŸ’¬ Simulating multi-turn conversation with Semantic Cache

[API CALL #1] What is the impact of climate change on corn yields?
â†’ LLM Response: Climate change is having a significant impact...

[CACHE HIT #2] Effect of global warming on corn productivity
â†’ Return cached content: Climate change is having a signific
```
